{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common.ipynb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_token=os.getenv('JIRA_TOKEN')\n",
    "\n",
    "if not api_token:\n",
    "    raise ValueError(\"No JIRA API token found. Check your .env file / environment variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run_proj = sys.argv[1]\n",
    "run_proj ='ALL'\n",
    "\n",
    "if run_proj == 'SOLE':\n",
    "    print('SOLE')\n",
    "    partitionCnt=1\n",
    "    filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/SOLE/'\n",
    "else:\n",
    "    print('ALL')\n",
    "    partitionCnt=30\n",
    "    filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/AllProjects/'\n",
    "\n",
    "# filepath='/Users/u1002018/src/PAE/JIRA_PAE/output/.'\n",
    "\n",
    "\n",
    "datestr = \"\"\n",
    "\n",
    "if run_proj == 'SOLE' :\n",
    "    issueLists = [\n",
    "            #   {'issueType':\"Portfolio Initiative\",'partitionCnt':1},\n",
    "            #   {'issueType':\"Product Initiative\", 'partitionCnt':1 },\n",
    "              {'issueType':\"Epic\",'partitionCnt':5},\n",
    "              # {'issueType':\"Story\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "\n",
    "             ]\n",
    "else:\n",
    "    issueLists = [\n",
    "            #   {'issueType':\"Portfolio Initiative\",'partitionCnt':1},\n",
    "            #   {'issueType':\"Product Initiative\", 'partitionCnt':1 },\n",
    "              {'issueType':\"Epic\",'partitionCnt':5},\n",
    "              # {'issueType':\"Story\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "\n",
    "             ]\n",
    "             \n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDataframe():\n",
    "    return pd.DataFrame (columns=['key','FixVersion', \n",
    "    'Priority',\n",
    "    'Acceptance Criteria',\n",
    "    'Severity Level',\n",
    "    'Labels',\n",
    "    'EpicLink',\n",
    "    # 'Issuelinks',\n",
    "    'Status',\n",
    "    'Components',\n",
    "    'IssueCreator',\n",
    "    'creator.emailAddress',\n",
    "    'creator.displayName',\n",
    "    #  'SubTasks',\n",
    "    'Reporter.name',\n",
    "    'reporter.emailAddress',\n",
    "    'reporter.displayName',\n",
    "    'issueType',\n",
    "    'projectKey',\n",
    "    'projectName',\n",
    "    'created_ts',\n",
    "    'updated_ts',\n",
    "    'ParentLink',\n",
    "    'summary',\n",
    "    'StoryPoint',\n",
    "    'Sprint', \n",
    "    'AssigneeName',\n",
    "    'AssigneeEmailAddress',\n",
    "    'AssigneeDisplayName',\n",
    "    'ResolutionDescription',\n",
    "    'ResolutionName',\n",
    "    'parent.Summary',\n",
    "    'parent.key',\n",
    "    'parent.status',\n",
    "    'parent.priority',\n",
    "    'Defect Type',\n",
    "    'Test Cycle',\n",
    "    'Bug Severity',\n",
    "    'Sub-track',\n",
    "    'RICE Object Type',\n",
    "    'Complexity',\n",
    "    'Scope',\n",
    "    'Baseline Scope',\n",
    "    'Sprint_state',\n",
    "    'Sprint_name',\n",
    "    'Sprint_startDate',\n",
    "    'Sprint_endDate',\n",
    "    'Sprint_completeDate',        \n",
    "    'Sprint_activatedDate',\n",
    "    'Sprint_sequence',\n",
    "    'Sprint_goal',\n",
    "    'Intial SOW',\n",
    "    'After Global Design',\n",
    "    'The Rudy Special',\n",
    "    'Changelog',\n",
    "    'CT_leadTime',\n",
    "    'CT_development',\n",
    "    'CT_discovery',\n",
    "    'CT_deployment',\n",
    "    'CT_cancelled',\n",
    "    'CT_qa',\n",
    "    'Changelog_Rec',\n",
    "    'Epic Type',\n",
    "    'Target start',\n",
    "    'Target end',\n",
    "    'T-Shirt size',\n",
    "    'Affected Banners',\n",
    "    'Affected Geographies'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 08:28:25,541 - root - INFO - =======================================\n",
      "2025-01-18 08:28:25,542 - root - INFO - Script start accumulating data from JIRA\n"
     ]
    }
   ],
   "source": [
    "global logger\n",
    "logger = setup_logger(logging.INFO)\n",
    "logger.info(\"=======================================\")\n",
    "logger.info(\"Script start accumulating data from JIRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(dataframe):\n",
    "    if dataframe is not None and not dataframe.empty:\n",
    "       dataframe= dataframe.filter(['key','fields.fixVersions',\n",
    "                    'fields.priority.name',\n",
    "                    'fields.customfield_31601',\n",
    "                    'fields.customfield_12402.value',\n",
    "                    'fields.labels',\n",
    "                    'fields.customfield_10006',\n",
    "                    # 'fields.issuelinks',\n",
    "                    'fields.status.name',\n",
    "                    'fields.components',\n",
    "                    'fields.creator.name',\n",
    "                    'fields.creator.emailAddress',\n",
    "                    'fields.creator.displayName',\n",
    "                    # 'fields.subtasks',\n",
    "                    'fields.reporter.name',\n",
    "                    'fields.reporter.emailAddress',\n",
    "                    'fields.reporter.displayName',\n",
    "                    'fields.issuetype.name',\n",
    "                    'fields.project.key',\n",
    "                    'fields.project.name',\n",
    "                    'fields.created',\n",
    "                    'fields.updated',\n",
    "                    'fields.customfield_12823',\n",
    "                    'fields.summary',\n",
    "                    'fields.customfield_10002',\n",
    "                    'fields.customfield_10004',\n",
    "                    'fields.assignee.name',\n",
    "                    'fields.assignee.emailAddress',\n",
    "                    'fields.assignee.displayName',\n",
    "                    'fields.resolution.description',\n",
    "                    'fields.resolution.name',\n",
    "                    'fields.parent.fields.summary',\n",
    "                    'fields.parent.key',\n",
    "                    'fields.parent.fields.status.name',\n",
    "                    'fields.parent.fields.priority.name',\n",
    "                    'fields.customfield_32101.value',\n",
    "                    'fields.customfield_32302.value',\n",
    "                    'fields.customfield_11605.value',\n",
    "                    'fields.customfield_32901',\n",
    "                    # 'fields.customfield_16902' ,\n",
    "                    'fields.customfield_32900.value' ,\n",
    "                    'fields.customfield_27503' ,\n",
    "                    'fields.customfield_16902.value',\n",
    "                    'fields.customfield_33000',\n",
    "                    'fields.customfield_31400.value',\n",
    "                    'fields.customfield_13800',\n",
    "                    'fields.customfield_13801',\n",
    "                    'fields.customfield_10207.value',\n",
    "                    'fields.customfield_17210',\n",
    "                    'fields.customfield_17211',\n",
    "                    'changelog.histories']).rename(columns={'fields.fixVersions': 'FixVersion', \n",
    "                                  'fields.priority.name':'Priority',\n",
    "                                  'fields.customfield_31601': 'Acceptance Criteria',\n",
    "                                  'fields.customfield_12402.value':'Severity Level',\n",
    "                                  'fields.labels':'Labels',\n",
    "                                  'fields.customfield_10006':'EpicLink',\n",
    "                                #   'fields.issuelinks': 'Issuelinks',\n",
    "                                  'fields.status.name': 'Status',\n",
    "                                  'fields.components':'Components',\n",
    "                                  'fields.creator.name':'IssueCreator',\n",
    "                                  'fields.creator.emailAddress':'creator.emailAddress',\n",
    "                                  'fields.creator.displayName': 'creator.displayName',\n",
    "                                  # 'fields.subtasks':'SubTasks',\n",
    "                                  'fields.reporter.name':'Reporter.name',\n",
    "                                  'fields.reporter.emailAddress':'reporter.emailAddress',\n",
    "                                  'fields.reporter.displayName': 'reporter.displayName',\n",
    "                                  'fields.issuetype.name':'issueType',\n",
    "                                  'fields.project.key':'projectKey',\n",
    "                                  'fields.project.name':'projectName',\n",
    "                                  'fields.created':'created_ts',\n",
    "                                  'fields.updated':'updated_ts',\n",
    "                                  'fields.customfield_12823': 'ParentLink',\n",
    "                                  'fields.summary':'summary',\n",
    "                                  'fields.customfield_10002':'StoryPoint',\n",
    "                                  'fields.customfield_10004':'Sprint',                                  \n",
    "                                  'fields.assignee.name':'AssigneeName',\n",
    "                                  'fields.assignee.emailAddress':'AssigneeEmailAddress',\n",
    "                                  'fields.assignee.displayName':'AssigneeDisplayName',\n",
    "                                  'fields.resolution.description':'ResolutionDescription',\n",
    "                                  'fields.resolution.name':'ResolutionName',\n",
    "                                  'fields.parent.fields.summary': 'parent.Summary',\n",
    "                                  'fields.parent.key': 'parent.key',\n",
    "                                  'fields.parent.fields.status.name': 'parent.status',\n",
    "                                  'fields.parent.fields.priority.name':'parent.priority',\n",
    "                                  'fields.customfield_32101.value':'Defect Type',\n",
    "                                  'fields.customfield_32302.value':'Test Cycle',\n",
    "                                  'fields.customfield_11605.value':'Bug Severity',\n",
    "                                  'fields.customfield_32901':'Sub-track',\n",
    "                                  'fields.customfield_32900.value' : 'RICE Object Type',\n",
    "                                  'fields.customfield_27503' : 'Complexity',\n",
    "                                  'fields.customfield_16902.value':'Scope',\n",
    "                                  'fields.customfield_33000':'Baseline Scope',\n",
    "                                  'changelog.histories':'Changelog',\n",
    "                                  'fields.customfield_31400.value': 'Epic Type',\n",
    "                                  'fields.customfield_13800': 'Target start',\n",
    "                                  'fields.customfield_13801': 'Target end',\n",
    "                                  'fields.customfield_10207.value':'T-Shirt size',\n",
    "                                  'fields.customfield_17210':'Affected Banners',\n",
    "                                  'fields.customfield_17211':'Affected Geographies'\n",
    "                                  })\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch( project, issueType):\n",
    "    global fetch_df \n",
    "    logger.info(f\"Fetch {issueType}...\")\n",
    "\n",
    "    if issueType == 'Story' or issueType =='Epic':\n",
    "        attribute =' &expand=projects.issuetypes.fields,changelog'\n",
    "    else:\n",
    "        attribute =' &expand=projects.issuetypes.fields'\n",
    "\n",
    "    searchQry=f'?jql= project in ({project}) and issueType=\"{issueType}\" and updated > startOfMonth(-11)'\n",
    "\n",
    "    # if cond_flg == True: \n",
    "    #     searchQry=f\"{searchQry} and status not in (Cancelled, Done)  {attribute}\"\n",
    "    # else:\n",
    "    searchQry=f\"{searchQry} {attribute}\"\n",
    "\n",
    "    fetch_df = getDataSet(searchQry,project, issueType, api_token )\n",
    "\n",
    "    return data_clean(fetch_df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSprintDetails(l_Df):\n",
    "    l_Df['Sprint'] = l_Df['Sprint'].apply(lambda x: convert_to_key_value(x))\n",
    "\n",
    "    l_Df['Sprint_state'] = l_Df['Sprint'].apply(lambda x: sprintvalue(x,'state') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_name']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'name') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_startDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'startDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_endDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'endDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_completeDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'completeDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_activatedDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'activatedDate') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_sequence']= epic_Df['Sprint'].apply(lambda x: sprintvalue(x,'sequence') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_goal']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'goal') if x is not None else \"NA\" )\n",
    "    l_Df= l_Df.drop(columns=['Sprint'])\n",
    "\n",
    "    return l_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeLog(element):\n",
    "    changelog_Rec = [] \n",
    "    for history in element:\n",
    "        created_at = history.get('created',None)\n",
    "        for item in history['items']:\n",
    "            if item['field'] =='status' :\n",
    "                changelog_Rec.append({\n",
    "                    'created_at': created_at,\n",
    "                    'field': item['field'],\n",
    "                    'from': item['fromString'],\n",
    "                    'to': item['toString'],\n",
    "                })\n",
    "    return changelog_Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCycleTime(l_Df):\n",
    "    cycleTime = {}\n",
    "    for index, row in l_Df.iterrows():\n",
    "        # display(row)\n",
    "        _status_change = pd.json_normalize(row['Changelog_Rec'])\n",
    "        if len(_status_change) > 0:\n",
    "            logger.debug(row['Status'])\n",
    "            if (row['Status'] not in close_state):\n",
    "                # Get the current time in UTC\n",
    "                now_utc = datetime.now(pytz.utc)\n",
    "                # Convert to Eastern Time\n",
    "                eastern = pytz.timezone('US/Eastern')\n",
    "                now_est = now_utc.astimezone(eastern)\n",
    "                new_node = {\n",
    "                            'created_at': now_est.strftime('%Y-%m-%dT%H:%M:%S.%f%z'),\n",
    "                             'field': 'status',\n",
    "                             'from': row['Status'],\n",
    "                             'to' : 'Active'\n",
    "                            #  'proj'=row['projectKey'],\n",
    "                            #  'key'=row['key']\n",
    "\n",
    "                            }\n",
    "                _status_change.loc[len(_status_change)] = new_node\n",
    "\n",
    "            newCreate_node = {\n",
    "                        # 'created_at': row['created_ts'].to_list()[0], \n",
    "                            'created_at': row['created_ts'],\n",
    "                            'field': 'status',\n",
    "                            'from': 'created',\n",
    "                            'to' : 'WorkflowState'\n",
    "                            # 'proj'=row['projectKey'],\n",
    "                            # 'key'=row['key']\n",
    "                        }\n",
    "\n",
    "            _status_change.loc[len(_status_change)] = newCreate_node   \n",
    "            _status_change.sort_values(by=['created_at'],inplace=True)\n",
    "            _status_change['created_at']= pd.to_datetime(_status_change['created_at'])\n",
    "            _status_change['time_diff'] = _status_change['created_at'].diff()\n",
    "\n",
    "\n",
    "\n",
    "            l_Df.at[index,'CT_leadTime'] = format_timedelta(_status_change['time_diff'].sum())\n",
    "\n",
    "            l_Df.at[index,'CT_discovery'] = format_timedelta(_status_change[_status_change['to'].isin( discovery_state )]['time_diff'].sum())\n",
    "            \n",
    "            l_Df.at[index,'CT_development'] = format_timedelta(_status_change[_status_change['to'].isin( development_state)]['time_diff'].sum())                                        \n",
    "            l_Df.at[index,'CT_deployment'] = format_timedelta(_status_change[_status_change['to'].isin(deployment_state)]['time_diff'].sum())\n",
    "            l_Df.at[index,'CT_cancelled'] = format_timedelta(_status_change[_status_change['to'].isin(cancel_state )]['time_diff'].sum())   \n",
    "            l_Df.at[index,'CT_qa'] = format_timedelta(_status_change[_status_change['to'].isin(qa_state)]['time_diff'].sum())\n",
    "            \n",
    "            _status_change['created_at'] = pd.to_datetime(_status_change['created_at'],utc=True)\n",
    "            _status_change['created_at'] = _status_change['created_at'].dt.date\n",
    "            _status_change['proj']=row['projectKey']\n",
    "            _status_change['key']=row['key']\n",
    "            \n",
    "            # display(_status_change)\n",
    "            output_path = os.path.join(filepath, 'transition_history_Story_working.csv')\n",
    "            _status_change.to_csv(output_path ,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "\n",
    "    display(l_Df['CT_leadTime'])       \n",
    "    return l_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 08:28:26,298 - root - INFO - Getting report results...\n",
      "2025-01-18 08:28:26,298 - root - INFO - https://jira.footlocker.com/rest/api/2/project\n",
      "2025-01-18 08:28:27,620 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:28:27,621 - root - INFO - Report results received...\n",
      "2025-01-18 08:28:27,621 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:28:27,627 - root - WARNING - processing TEST - Epic\n",
      "2025-01-18 08:28:27,627 - root - INFO - Fetch Epic...\n",
      "2025-01-18 08:28:27,628 - root - INFO - Getting report results...\n",
      "2025-01-18 08:28:27,628 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"TEST\", \"CCEP\", \"CHAT\", \"COMPRODUCT\", \"AEMSC\", \"AWFM\", \"AII2\", \"AT\", \"BAE\", \"BOT\", \"BRWSE\", \"CHSC\", \"CNC\", \"CE\", \"CPENG\", \"CCOE\", \"CSP\", \"CA\", \"CMST\", \"CROHR\", \"CROPS\", \"COBUSS\", \"CEA\", \"CCL\", \"CEO\", \"CSZI\", \"CXCD\", \"INTEL\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=0\n",
      "2025-01-18 08:31:08,801 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:31:08,802 - root - INFO - Report results received...\n",
      "2025-01-18 08:31:08,803 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:31:09,169 - root - INFO - total: 494 (\"TEST\", \"CCEP\", \"CHAT\", \"COMPRODUCT\", \"AEMSC\", \"AWFM\", \"AII2\", \"AT\", \"BAE\", \"BOT\", \"BRWSE\", \"CHSC\", \"CNC\", \"CE\", \"CPENG\", \"CCOE\", \"CSP\", \"CA\", \"CMST\", \"CROHR\", \"CROPS\", \"COBUSS\", \"CEA\", \"CCL\", \"CEO\", \"CSZI\", \"CXCD\", \"INTEL\" - Epic)\n",
      "2025-01-18 08:31:09,305 - root - INFO - Files uploaded\n",
      "2025-01-18 08:31:09,305 - root - INFO - Working on Stories...\n",
      "2025-01-18 08:31:09,306 - root - INFO - Upload file to sharepoint...\n",
      "2025-01-18 08:31:09,313 - root - INFO - Move working files to current files\n",
      "2025-01-18 08:31:09,427 - root - WARNING - processing CLAIMS - Epic\n",
      "2025-01-18 08:31:09,427 - root - INFO - Fetch Epic...\n",
      "2025-01-18 08:31:09,428 - root - INFO - Getting report results...\n",
      "2025-01-18 08:31:09,428 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=0\n",
      "2025-01-18 08:33:32,669 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:33:32,670 - root - INFO - Report results received...\n",
      "2025-01-18 08:33:32,670 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:33:32,800 - root - INFO - total: 1122 (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\" - Epic)\n",
      "2025-01-18 08:33:32,852 - root - INFO - Getting report results...\n",
      "2025-01-18 08:33:32,852 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=500\n",
      "2025-01-18 08:35:53,956 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:35:53,957 - root - INFO - Report results received...\n",
      "2025-01-18 08:35:53,957 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:35:54,089 - root - INFO - total: 1122 (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\" - Epic)\n",
      "2025-01-18 08:35:54,172 - root - INFO - Getting report results...\n",
      "2025-01-18 08:35:54,172 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=1000\n",
      "2025-01-18 08:36:31,778 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:36:31,779 - root - INFO - Report results received...\n",
      "2025-01-18 08:36:31,779 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:36:31,897 - root - INFO - total: 1122 (\"CLAIMS\", \"DNA\", \"DI\", \"DTIB\", \"DP\", \"DTD\", \"DCTEU\", \"DCTINT\", \"DCTOM\", \"DWP\", \"DAAS\", \"DPE\", \"DD\", \"DPT\", \"DWE\", \"DR\", \"DOM\", \"DOMSIT\", \"DOMUAT\", \"SII\", \"EADEVOPS\", \"EAFIN\", \"ESB\", \"EWFM\", \"EUS\", \"EVM\", \"EA\", \"ENT\" - Epic)\n",
      "2025-01-18 08:36:31,981 - root - INFO - Files uploaded\n",
      "2025-01-18 08:36:31,982 - root - INFO - Working on Stories...\n",
      "2025-01-18 08:36:31,982 - root - INFO - Upload file to sharepoint...\n",
      "2025-01-18 08:36:32,001 - root - INFO - Move working files to current files\n",
      "2025-01-18 08:36:32,125 - root - WARNING - processing EUCF - Epic\n",
      "2025-01-18 08:36:32,126 - root - INFO - Fetch Epic...\n",
      "2025-01-18 08:36:32,126 - root - INFO - Getting report results...\n",
      "2025-01-18 08:36:32,126 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"EUCF\", \"LOGISTICS\", \"EXV\", \"FEQA\", \"FIN\", \"FLDBA\", \"FLINV\", \"FLAIR\", \"FLAPI\", \"FLXFR\", \"FLXME\", \"FLXP\", \"FED\", \"GCPM\", \"AUTOPS\", \"DATAOPS\", \"INTOPS\", \"GOBEIP\", \"ICOE\", \"IL\", \"INV\", \"IPCM\", \"AUTO\", \"ITSM\", \"RESO\", \"MARTECH\", \"HAPS\", \"MA\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=0\n",
      "2025-01-18 08:37:33,308 - root - INFO - <Response [502]>\n",
      "2025-01-18 08:37:33,309 - root - ERROR - HTTP 502 - Bad Gateway\n",
      "2025-01-18 08:40:13,239 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:40:13,240 - root - INFO - Report results received...\n",
      "2025-01-18 08:40:13,240 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:40:13,426 - root - INFO - total: 560 (\"EUCF\", \"LOGISTICS\", \"EXV\", \"FEQA\", \"FIN\", \"FLDBA\", \"FLINV\", \"FLAIR\", \"FLAPI\", \"FLXFR\", \"FLXME\", \"FLXP\", \"FED\", \"GCPM\", \"AUTOPS\", \"DATAOPS\", \"INTOPS\", \"GOBEIP\", \"ICOE\", \"IL\", \"INV\", \"IPCM\", \"AUTO\", \"ITSM\", \"RESO\", \"MARTECH\", \"HAPS\", \"MA\" - Epic)\n",
      "2025-01-18 08:40:13,479 - root - INFO - Getting report results...\n",
      "2025-01-18 08:40:13,479 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"EUCF\", \"LOGISTICS\", \"EXV\", \"FEQA\", \"FIN\", \"FLDBA\", \"FLINV\", \"FLAIR\", \"FLAPI\", \"FLXFR\", \"FLXME\", \"FLXP\", \"FED\", \"GCPM\", \"AUTOPS\", \"DATAOPS\", \"INTOPS\", \"GOBEIP\", \"ICOE\", \"IL\", \"INV\", \"IPCM\", \"AUTO\", \"ITSM\", \"RESO\", \"MARTECH\", \"HAPS\", \"MA\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=500\n",
      "2025-01-18 08:40:26,512 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:40:26,513 - root - INFO - Report results received...\n",
      "2025-01-18 08:40:26,514 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:40:26,545 - root - INFO - total: 560 (\"EUCF\", \"LOGISTICS\", \"EXV\", \"FEQA\", \"FIN\", \"FLDBA\", \"FLINV\", \"FLAIR\", \"FLAPI\", \"FLXFR\", \"FLXME\", \"FLXP\", \"FED\", \"GCPM\", \"AUTOPS\", \"DATAOPS\", \"INTOPS\", \"GOBEIP\", \"ICOE\", \"IL\", \"INV\", \"IPCM\", \"AUTO\", \"ITSM\", \"RESO\", \"MARTECH\", \"HAPS\", \"MA\" - Epic)\n",
      "2025-01-18 08:40:26,661 - root - INFO - Files uploaded\n",
      "2025-01-18 08:40:26,661 - root - INFO - Working on Stories...\n",
      "2025-01-18 08:40:26,661 - root - INFO - Upload file to sharepoint...\n",
      "2025-01-18 08:40:26,681 - root - INFO - Move working files to current files\n",
      "2025-01-18 08:40:26,780 - root - WARNING - processing MEMBR - Epic\n",
      "2025-01-18 08:40:26,781 - root - INFO - Fetch Epic...\n",
      "2025-01-18 08:40:26,781 - root - INFO - Getting report results...\n",
      "2025-01-18 08:40:26,782 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"MEMBR\", \"QAS\", \"MERCH\", \"M365G\", \"MAD\", \"MAVEN\", \"NS\", \"OEPORT\", \"FLXO\", \"PAYAC\", \"PTF\", \"PT\", \"PIM\", \"PCR\", \"PE\", \"PPCL\", \"PWG\", \"PTAG\", \"PAI\", \"PAE\", \"PNC\", \"PDI\", \"CIAM\", \"LKSIDE\", \"ROI\", \"RTVREG\", \"SCHED\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=0\n",
      "2025-01-18 08:41:52,252 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:41:52,253 - root - INFO - Report results received...\n",
      "2025-01-18 08:41:52,253 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:41:52,367 - root - INFO - total: 687 (\"MEMBR\", \"QAS\", \"MERCH\", \"M365G\", \"MAD\", \"MAVEN\", \"NS\", \"OEPORT\", \"FLXO\", \"PAYAC\", \"PTF\", \"PT\", \"PIM\", \"PCR\", \"PE\", \"PPCL\", \"PWG\", \"PTAG\", \"PAI\", \"PAE\", \"PNC\", \"PDI\", \"CIAM\", \"LKSIDE\", \"ROI\", \"RTVREG\", \"SCHED\" - Epic)\n",
      "2025-01-18 08:41:52,424 - root - INFO - Getting report results...\n",
      "2025-01-18 08:41:52,424 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"MEMBR\", \"QAS\", \"MERCH\", \"M365G\", \"MAD\", \"MAVEN\", \"NS\", \"OEPORT\", \"FLXO\", \"PAYAC\", \"PTF\", \"PT\", \"PIM\", \"PCR\", \"PE\", \"PPCL\", \"PWG\", \"PTAG\", \"PAI\", \"PAE\", \"PNC\", \"PDI\", \"CIAM\", \"LKSIDE\", \"ROI\", \"RTVREG\", \"SCHED\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=500\n",
      "2025-01-18 08:44:26,301 - root - INFO - <Response [200]>\n",
      "2025-01-18 08:44:26,301 - root - INFO - Report results received...\n",
      "2025-01-18 08:44:26,302 - root - INFO - HTTP 200 - OK\n",
      "2025-01-18 08:44:26,422 - root - INFO - total: 687 (\"MEMBR\", \"QAS\", \"MERCH\", \"M365G\", \"MAD\", \"MAVEN\", \"NS\", \"OEPORT\", \"FLXO\", \"PAYAC\", \"PTF\", \"PT\", \"PIM\", \"PCR\", \"PE\", \"PPCL\", \"PWG\", \"PTAG\", \"PAI\", \"PAE\", \"PNC\", \"PDI\", \"CIAM\", \"LKSIDE\", \"ROI\", \"RTVREG\", \"SCHED\" - Epic)\n",
      "2025-01-18 08:44:26,542 - root - INFO - Files uploaded\n",
      "2025-01-18 08:44:26,542 - root - INFO - Working on Stories...\n",
      "2025-01-18 08:44:26,542 - root - INFO - Upload file to sharepoint...\n",
      "2025-01-18 08:44:26,566 - root - INFO - Move working files to current files\n",
      "2025-01-18 08:44:26,679 - root - WARNING - processing SND - Epic\n",
      "2025-01-18 08:44:26,680 - root - INFO - Fetch Epic...\n",
      "2025-01-18 08:44:26,680 - root - INFO - Getting report results...\n",
      "2025-01-18 08:44:26,680 - root - INFO - https://jira.footlocker.com/rest/api/2/search?jql= project in (\"SND\", \"SNB\", \"ITSEC\", \"SECSE\", \"SERNOW\", \"SHP\", \"SO\", \"SAM\", \"SOLEFIN\", \"SOLMERCH\", \"SSP\", \"SSE\", \"PRODSUPP\", \"SCM\", \"SPIM\", \"TOPS\", \"TECH\", \"TPCR\", \"TPDI\", \"WHSYS\", \"UXD\", \"VDI\", \"VMO\", \"WCDI\", \"WFM\", \"XSTR\", \"ZGA\") and issueType=\"Epic\" and updated > startOfMonth(-11)  &expand=projects.issuetypes.fields,changelog&maxResults=500&startAt=0\n",
      "2025-01-18 08:45:29,330 - root - INFO - <Response [502]>\n",
      "2025-01-18 08:45:29,332 - root - ERROR - HTTP 502 - Bad Gateway\n",
      "2025-01-18 08:46:30,816 - root - INFO - <Response [502]>\n",
      "2025-01-18 08:46:30,818 - root - ERROR - HTTP 502 - Bad Gateway\n",
      "2025-01-18 08:47:33,426 - root - INFO - <Response [502]>\n",
      "2025-01-18 08:47:33,427 - root - ERROR - HTTP 502 - Bad Gateway\n",
      "2025-01-18 08:47:33,429 - root - ERROR - Dataset was empty (\"SND\", \"SNB\", \"ITSEC\", \"SECSE\", \"SERNOW\", \"SHP\", \"SO\", \"SAM\", \"SOLEFIN\", \"SOLMERCH\", \"SSP\", \"SSE\", \"PRODSUPP\", \"SCM\", \"SPIM\", \"TOPS\", \"TECH\", \"TPCR\", \"TPDI\", \"WHSYS\", \"UXD\", \"VDI\", \"VMO\", \"WCDI\", \"WFM\", \"XSTR\", \"ZGA\" - Epic)....\n",
      "2025-01-18 08:47:33,565 - root - WARNING - renaming Epic_working.csv TO Epic.csv....\n",
      "2025-01-18 08:47:33,568 - root - WARNING - Script ended accumulating data from JIRA \n",
      "2025-01-18 08:47:33,568 - root - WARNING - =======================================\n"
     ]
    }
   ],
   "source": [
    "# Get all .csv files in the 'data' directory\n",
    "csv_files = Path(filepath).glob(\"*_working.csv\")\n",
    "for file in csv_files:\n",
    "    filename=file.name\n",
    "    os.remove(os.path.join(filepath, filename))\n",
    "\n",
    "\n",
    "if run_proj == 'SOLE':\n",
    "   strings =pd.Series([\"PCR\"])\n",
    "   # strings =pd.Series([\"SOLMerch\",\"SOLEFIN\"])\n",
    "   \n",
    "else:\n",
    "   strings = projectList(api_token)\n",
    "\n",
    "for issueList in issueLists:\n",
    "    split_string_list = split_list(strings, issueList['partitionCnt'])\n",
    "    filtered_list = [series for series in split_string_list if not series.empty]\n",
    "    for partition in filtered_list:\n",
    "        logger.warning(f\"processing %s - %s\",partition.values[0],issueList['issueType'])\n",
    "        output_path = os.path.join(filepath, f\"{issueList['issueType']}_working.csv\")\n",
    "        df=fetch(', '.join(['\"{}\"'.format(value) for value in partition]),issueList['issueType'])\n",
    "        if df is not None and not df.empty:\n",
    "            g_Df = initDataframe()\n",
    "            g_Df = pd.concat([g_Df, df ], ignore_index=True)\n",
    "            g_Df = extractSprintDetails(g_Df)\n",
    "            g_Df=g_Df.fillna(\"NA\")\n",
    "\n",
    "            g_Df['FixVersion']= g_Df['FixVersion'].apply(lambda x: keyvalue(x,'name') if x is not None else \"\" )\n",
    "            if run_proj == 'SOLE':\n",
    "               g_Df['Baseline Scope']=g_Df['Baseline Scope'].apply(lambda x: getBaselineLst(x) )\n",
    "               g_Df['Intial SOW']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 1')] )\n",
    "               g_Df['After Global Design']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 2')] )\n",
    "               g_Df['The Rudy Special']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 3')] )\n",
    "            \n",
    "            # display(g_Df)\n",
    "            # g_Df['Issuelinks'] = g_Df['key'].apply(lambda x:getLinkedIssues(x))\n",
    "\n",
    "            logger.info(\"Files uploaded\")\n",
    "\n",
    "            logger.info(\"Working on Stories...\")\n",
    "            if issueList['issueType'] == 'Story':\n",
    "               g_Df['Changelog_Rec'] = g_Df['Changelog'].apply(lambda x: getChangeLog(x) )\n",
    "               g_Df = calculateCycleTime(g_Df)\n",
    "               if run_proj == 'SOLE':\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','Components'])\n",
    "               else:\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','summary','Components'])\n",
    "               try:\n",
    "                  g_Df[g_Df['issueType'] == 'Story'].to_csv(output_path,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "               except Exception as e:  \n",
    "                  logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "                  pass\n",
    "            else:\n",
    "               logger.info(\"Upload file to sharepoint...\")\n",
    "               if run_proj == 'SOLE':\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','Components'])\n",
    "               else:\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Labels','summary','Components'])\n",
    "               try:\n",
    "                  if issueList['issueType'] in ['Bug','Incident','Production Defects','Defect','Issue']:\n",
    "                     output_path = os.path.join(filepath, 'Bug_working.csv')\n",
    "                     g_Df.loc[g_Df['issueType'].isin(['Bug','Incident','Production Defects','Defect','Issue'])].to_csv(output_path, mode='a', header=not os.path.exists(output_path),index=False)\n",
    "                  else:                  \n",
    "                     g_Df[g_Df['issueType'] == issueList['issueType']].to_csv(output_path ,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "                  \n",
    "               except Exception as e:  \n",
    "                  logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "                  pass\n",
    "\n",
    "            logger.info(\"Move working files to current files\")\n",
    "            # Delete the old DataFrame \n",
    "            del(g_Df)\n",
    "\n",
    "            # Perform garbage collection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "            gc.collect()\n",
    "            # break\n",
    "    clean_folder(filepath,\"_working.csv\")\n",
    "logger.warning(f\"Script ended accumulating data from JIRA \")\n",
    "logger.warning(\"=======================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fb6657aaf366e700473b4691a0218895a79fd82ea30347f6976080edd370586"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
