{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common.ipynb\n",
    "\n",
    "\n",
    "# filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/SOLE/'\n",
    "filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/AllProjects/'\n",
    "# filepath='./output/'\n",
    "\n",
    "\n",
    "datestr = \"\"\n",
    "\n",
    "g_Df = pd.DataFrame (columns=['key','FixVersion', \n",
    " 'Priority',\n",
    " 'Acceptance Criteria',\n",
    " 'Severity Level',\n",
    " 'Labels',\n",
    " 'EpicLink',\n",
    " 'Issuelinks',\n",
    " 'Status',\n",
    " 'Components',\n",
    " 'IssueCreator',\n",
    " 'creator.emailAddress',\n",
    " 'creator.displayName',\n",
    "#  'SubTasks',\n",
    " 'Reporter.name',\n",
    " 'reporter.emailAddress',\n",
    " 'reporter.displayName',\n",
    " 'issueType',\n",
    " 'projectKey',\n",
    " 'projectName',\n",
    " 'created_ts',\n",
    " 'updated_ts',\n",
    " 'ParentLink',\n",
    " 'summary',\n",
    " 'StoryPoint',\n",
    " 'Sprint', \n",
    " 'AssigneeName',\n",
    " 'AssigneeEmailAddress',\n",
    " 'AssigneeDisplayName',\n",
    " 'ResolutionDescription',\n",
    " 'ResolutionName',\n",
    " 'parent.Summary',\n",
    " 'parent.key',\n",
    " 'parent.status',\n",
    " 'parent.priority',\n",
    " 'Defect Type',\n",
    " 'Test Cycle',\n",
    " 'Bug Severity',\n",
    " 'Sub-track',\n",
    " 'RICE Object Type',\n",
    " 'Complexity',\n",
    " 'Scope',\n",
    " 'Baseline Scope',\n",
    " 'Sprint_state',\n",
    " 'Sprint_name',\n",
    " 'Sprint_startDate',\n",
    " 'Sprint_endDate',\n",
    " 'Sprint_completeDate',        \n",
    " 'Sprint_activatedDate',\n",
    " 'Sprint_sequence',\n",
    " 'Sprint_goal',\n",
    " 'Intial SOW',\n",
    " 'After Global Design',\n",
    " 'The Rudy Special',\n",
    " 'Changelog',\n",
    " 'CT_leadTime',\n",
    " 'CT_development',\n",
    " 'CT_discovery',\n",
    " 'CT_deployment',\n",
    " 'CT_cancelled',\n",
    " 'Changelog_Rec'\n",
    " ])\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global logger\n",
    "logger = setup_logger()\n",
    "logger.info(\"=======================================\")\n",
    "logger.info(\"Script start accumulating data from JIRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(dataframe):\n",
    "    if dataframe is not None and not dataframe.empty:\n",
    "       dataframe= dataframe.filter(['key','fields.fixVersions',\n",
    "                    'fields.priority.name',\n",
    "                    'fields.customfield_31601',\n",
    "                    'fields.customfield_12402.value',\n",
    "                    'fields.labels',\n",
    "                    'fields.customfield_10006',\n",
    "                    # 'fields.issuelinks',\n",
    "                    'fields.status.name',\n",
    "                    'fields.components',\n",
    "                    'fields.creator.name',\n",
    "                    'fields.creator.emailAddress',\n",
    "                    'fields.creator.displayName',\n",
    "                    # 'fields.subtasks',\n",
    "                    'fields.reporter.name',\n",
    "                    'fields.reporter.emailAddress',\n",
    "                    'fields.reporter.displayName',\n",
    "                    'fields.issuetype.name',\n",
    "                    'fields.project.key',\n",
    "                    'fields.project.name',\n",
    "                    'fields.created',\n",
    "                    'fields.updated',\n",
    "                    'fields.customfield_12823',\n",
    "                    'fields.summary',\n",
    "                    'fields.customfield_10002',\n",
    "                    'fields.customfield_10004',\n",
    "                    'fields.assignee.name',\n",
    "                    'fields.assignee.emailAddress',\n",
    "                    'fields.assignee.displayName',\n",
    "                    'fields.resolution.description',\n",
    "                    'fields.resolution.name',\n",
    "                    'fields.parent.fields.summary',\n",
    "                    'fields.parent.key',\n",
    "                    'fields.parent.fields.status.name',\n",
    "                    'fields.parent.fields.priority.name',\n",
    "                    'fields.customfield_32101.value',\n",
    "                    'fields.customfield_32302.value',\n",
    "                    'fields.customfield_11605.value',\n",
    "                    'fields.customfield_32901',\n",
    "                    # 'fields.customfield_16902' ,\n",
    "                    'fields.customfield_32900.value' ,\n",
    "                    'fields.customfield_27503' ,\n",
    "                    'fields.customfield_16902.value',\n",
    "                    'fields.customfield_33000',\n",
    "                    'changelog.histories']).rename(columns={'fields.fixVersions': 'FixVersion', \n",
    "                                  'fields.priority.name':'Priority',\n",
    "                                  'fields.customfield_31601': 'Acceptance Criteria',\n",
    "                                  'fields.customfield_12402.value':'Severity Level',\n",
    "                                  'fields.labels':'Labels',\n",
    "                                  'fields.customfield_10006':'EpicLink',\n",
    "                                #   'fields.issuelinks': 'Issuelinks',\n",
    "                                  'fields.status.name': 'Status',\n",
    "                                  'fields.components':'Components',\n",
    "                                  'fields.creator.name':'IssueCreator',\n",
    "                                  'fields.creator.emailAddress':'creator.emailAddress',\n",
    "                                  'fields.creator.displayName': 'creator.displayName',\n",
    "                                  # 'fields.subtasks':'SubTasks',\n",
    "                                  'fields.reporter.name':'Reporter.name',\n",
    "                                  'fields.reporter.emailAddress':'reporter.emailAddress',\n",
    "                                  'fields.reporter.displayName': 'reporter.displayName',\n",
    "                                  'fields.issuetype.name':'issueType',\n",
    "                                  'fields.project.key':'projectKey',\n",
    "                                  'fields.project.name':'projectName',\n",
    "                                  'fields.created':'created_ts',\n",
    "                                  'fields.updated':'updated_ts',\n",
    "                                  'fields.customfield_12823': 'ParentLink',\n",
    "                                  'fields.summary':'summary',\n",
    "                                  'fields.customfield_10002':'StoryPoint',\n",
    "                                  'fields.customfield_10004':'Sprint',                                  \n",
    "                                  'fields.assignee.name':'AssigneeName',\n",
    "                                  'fields.assignee.emailAddress':'AssigneeEmailAddress',\n",
    "                                  'fields.assignee.displayName':'AssigneeDisplayName',\n",
    "                                  'fields.resolution.description':'ResolutionDescription',\n",
    "                                  'fields.resolution.name':'ResolutionName',\n",
    "                                  'fields.parent.fields.summary': 'parent.Summary',\n",
    "                                  'fields.parent.key': 'parent.key',\n",
    "                                  'fields.parent.fields.status.name': 'parent.status',\n",
    "                                  'fields.parent.fields.priority.name':'parent.priority',\n",
    "                                  'fields.customfield_32101.value':'Defect Type',\n",
    "                                  'fields.customfield_32302.value':'Test Cycle',\n",
    "                                  'fields.customfield_11605.value':'Bug Severity',\n",
    "                                  'fields.customfield_32901':'Sub-track',\n",
    "                                  'fields.customfield_32900.value' : 'RICE Object Type',\n",
    "                                  'fields.customfield_27503' : 'Complexity',\n",
    "                                  'fields.customfield_16902.value':'Scope',\n",
    "                                  'fields.customfield_33000':'Baseline Scope',\n",
    "                                  'changelog.histories':'Changelog'\n",
    "                                  })\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch( project, issueType):\n",
    "    global fetch_df \n",
    "    logger.info(f\"Fetch {issueType}...\")\n",
    "\n",
    "    if issueType == 'Story':\n",
    "        attribute =' &expand=projects.issuetypes.fields,changelog'\n",
    "    else:\n",
    "        attribute =' &expand=projects.issuetypes.fields'\n",
    "\n",
    "    searchQry=f'?jql= project in ({project}) and issueType=\"{issueType}\"'\n",
    "\n",
    "    # if cond_flg == True: \n",
    "    #     searchQry=f\"{searchQry} and status not in (Cancelled, Done)  {attribute}\"\n",
    "    # else:\n",
    "    searchQry=f\"{searchQry} {attribute}\"\n",
    "\n",
    "    fetch_df = getDataSet(searchQry)\n",
    "\n",
    "    return data_clean(fetch_df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSprintDetails(l_Df):\n",
    "    l_Df['Sprint'] = l_Df['Sprint'].apply(lambda x: convert_to_key_value(x))\n",
    "\n",
    "    l_Df['Sprint_state'] = l_Df['Sprint'].apply(lambda x: sprintvalue(x,'state') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_name']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'name') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_startDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'startDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_endDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'endDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_completeDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'completeDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_activatedDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'activatedDate') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_sequence']= epic_Df['Sprint'].apply(lambda x: sprintvalue(x,'sequence') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_goal']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'goal') if x is not None else \"NA\" )\n",
    "    l_Df= l_Df.drop(columns=['Sprint'])\n",
    "\n",
    "    return l_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBaselineLst(element):\n",
    "   baselinelst =[]\n",
    "   if element != None:\n",
    "      if  element != 'NA':\n",
    "         for idx in element:\n",
    "            baselinelst.append(idx['value'])\n",
    "   return baselinelst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeLog(element):\n",
    "    changelog_Rec = [] \n",
    "    for history in element:\n",
    "        created_at = history.get('created',None)\n",
    "        for item in history['items']:\n",
    "            if item['field'] =='status' :\n",
    "                changelog_Rec.append({\n",
    "                    'created_at': created_at,\n",
    "                    'field': item['field'],\n",
    "                    'from': item['fromString'],\n",
    "                    'to': item['toString'],\n",
    "                })\n",
    "    return changelog_Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n):\n",
    "    \"\"\"Splits a list into n approximately equal parts.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    \n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings = projectList()\n",
    "# # strings ='\"SOLMerch\",\"SOLEFIN\"'\n",
    "# split_string_list = split_list(strings, issueList['partitionCnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings =pd.Series([\"SOLMerch\",\"SOLEFIN\"])\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_string_list = split_list(strings, 1)\n",
    "\n",
    "filtered_list = [series for series in split_string_list if not series.empty]\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionCnt=30\n",
    "issueLists = [\n",
    "              {'issueType':\"Portfolio Initiative\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Product Initiative\", 'partitionCnt':partitionCnt },\n",
    "              {'issueType':\"Epic\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Story\",'partitionCnt':partitionCnt}\n",
    "             ]\n",
    "\n",
    "# strings = projectList()\n",
    "strings =pd.Series([\"SOLMerch\",\"SOLEFIN\"])\n",
    "\n",
    "for issueList in issueLists:\n",
    "    split_string_list = split_list(strings, issueList['partitionCnt'])\n",
    "    filtered_list = [series for series in split_string_list if not series.empty]\n",
    "    for partition in filtered_list:\n",
    "        df=fetch(', '.join(['\"{}\"'.format(value) for value in partition]),issueList['issueType'])\n",
    "        if df is not None and not df.empty:\n",
    "            g_Df = pd.concat([g_Df, df ], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SOLE PROJECT ###########\n",
    "# issueLists = [\n",
    "#               {'issueType':\"Portfolio Initiative\"},\n",
    "#               {'issueType':\"Product Initiative\"},\n",
    "#               {'issueType':\"Epic\"},\n",
    "#               {'issueType':\"Story\"},\n",
    "#               {'issueType':\"Task\"},\n",
    "#               {'issueType':\"Sub-task\"},\n",
    "#               {'issueType':\"Bug\"},\n",
    "#               {'issueType':\"Incident\"},\n",
    "#               {'issueType':\"Production Defects\"},\n",
    "#               {'issueType':\"Defect\"},\n",
    "#               {'issueType':\"Issue\"},\n",
    "#               {'issueType':\"Test\"}\n",
    "#              ]\n",
    "# projects =[\"SOLMerch\",\"SOLEFIN\"]\n",
    "# # projects =[\"SOLMerch\"]\n",
    "# for project in projects:\n",
    "#     for issueList in issueLists:\n",
    "#         df=fetch(issueList['issueType'], project)\n",
    "#         if df is not None and not df.empty:\n",
    "#             g_Df = pd.concat([g_Df, df ], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_key_value(string):\n",
    "    result = {}\n",
    "   \n",
    "    if string :\n",
    "        pairs = string[0].split(',')  # Split string by comma\n",
    "        # logger.info(pairs)\n",
    "        idx =1\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')  # Split each pair by '='\n",
    "            result[key.strip()] = value.strip()  # Add to dictionary, stripping whitespace\n",
    "            idx = idx +1 \n",
    "            if idx > 9:\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Df = extractSprintDetails(g_Df)\n",
    "g_Df=g_Df.fillna(\"NA\")\n",
    "\n",
    "g_Df['FixVersion']= g_Df['FixVersion'].apply(lambda x: keyvalue(x,'name') if x is not None else \"\" )\n",
    "g_Df['Baseline Scope']=g_Df['Baseline Scope'].apply(lambda x: getBaselineLst(x) )\n",
    "g_Df['Intial SOW']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 1')] )\n",
    "g_Df['After Global Design']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 2')] )\n",
    "g_Df['The Rudy Special']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 3')] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_Df[g_Df['Intial SOW']=='YES'][['Intial SOW','After Global Design','The Rudy Special','Baseline Scope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Df['Changelog_Rec'] = g_Df['Changelog'].apply(lambda x: getChangeLog(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timedelta(td):\n",
    "    \"\"\"Formats a timedelta object to 'day.hours:min:sec' format.\"\"\"\n",
    "\n",
    "    seconds = td.total_seconds()\n",
    "    days, seconds = divmod(seconds, 86400)\n",
    "    hours, seconds = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "\n",
    "    return f\"{int(days)}.{int(hours):02}:{int(minutes):02}:{int(seconds):02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_state = [\"To Do\",\n",
    "            \"Backlog\",\n",
    "            \"Not Started\",\n",
    "            \"Ready\",\n",
    "            \"Refinement\",\n",
    "            \"Design\",\n",
    "            \"New\",\n",
    "            \"Drafting\",\n",
    "            \"In Review\",\n",
    "            \"Pending PO Review\",\n",
    "            \"Open\",\n",
    "            \"Pending\"\n",
    "            ]\n",
    "development_state = ['In Progress',\n",
    "                'On Hold',\n",
    "                'Rework',\n",
    "                'Fail',\n",
    "                'Build',\n",
    "                'Testing',\n",
    "                'Reopen',\n",
    "                'Work In Progress']\n",
    "deployment_state = ['Ready For Approval',\n",
    "                'Approved',\n",
    "                'Done',\n",
    "                'Delayed',\n",
    "                'Need Approval',\n",
    "                'PO Acceptance',\n",
    "                'Resolved',\n",
    "                'Closed Complete']\n",
    "\n",
    "cancel_state = ['Cancelled',\n",
    "        'Closed - Not Doing',\n",
    "        'Closed Skipped',\n",
    "        'Closed Incomplete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_state = ['Cancelled',\n",
    "        'Closed - Duplicate',\n",
    "        'Closed - Inactivity',\n",
    "        'Closed - Not Doing',\n",
    "        'Closed Skipped',\n",
    "        'Closed Incomplete','Closed','Done',\n",
    "        'Resolved',\n",
    "        'Closed Complete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCycleTime(l_Df):\n",
    "    cycleTime = {}\n",
    "    for index, row in l_Df.iterrows():\n",
    "        # display(row)\n",
    "        _status_change = pd.json_normalize(row['Changelog_Rec'])\n",
    "        if len(_status_change) > 0:\n",
    "            if (row['Status'] != 'Done'):\n",
    "                # Get the current time in UTC\n",
    "                now_utc = datetime.now(pytz.utc)\n",
    "                # Convert to Eastern Time\n",
    "                eastern = pytz.timezone('US/Eastern')\n",
    "                now_est = now_utc.astimezone(eastern)\n",
    "                new_node = {\n",
    "                            'created_at': now_est.strftime('%Y-%m-%dT%H:%M:%S.%f%z'),\n",
    "                             'field': 'status',\n",
    "                             'from': row['Status'],\n",
    "                             'to' : 'Active'\n",
    "                            }\n",
    "                _status_change.loc[len(_status_change)] = new_node\n",
    "\n",
    "            newCreate_node = {\n",
    "                        # 'created_at': row['created_ts'].to_list()[0], \n",
    "                            'created_at': row['created_ts'],\n",
    "                            'field': 'status',\n",
    "                            'from': 'created',\n",
    "                            'to' : 'WorkflowState'\n",
    "                        }\n",
    "            # _status_change['created_at'] = pd.to_datetime(_status_change['created_at'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "           \n",
    "            # _status_change= _status_change.tolist()\n",
    "            # _status_change= pd.json_normalize(_status_change[0])\n",
    "            _status_change.loc[len(_status_change)] = newCreate_node   \n",
    "            _status_change.sort_values(by=['created_at'],inplace=True)\n",
    "            _status_change['created_at']= pd.to_datetime(_status_change['created_at'])\n",
    "            _status_change['time_diff'] = _status_change['created_at'].diff()\n",
    "\n",
    "            # # _status_change['proj']=proj\n",
    "            # _status_change = _status_change.reset_index(drop=True)\n",
    "            # # _status_change.to_csv('./output/transistion_history.csv', mode='a', header=False, index=False)\n",
    "            # # cycleTime['proj'] = proj\n",
    "            # # cycleTime['key'] = _status_change['key'][0]\n",
    "            row['CT_leadTime'] = format_timedelta(_status_change['time_diff'].sum())\n",
    "\n",
    "            row['CT_discovery'] = format_timedelta(_status_change[_status_change['to'].isin( discovery_state )]['time_diff'].sum())\n",
    "            \n",
    "            row['CT_development'] = format_timedelta(_status_change[_status_change['to'].isin( development_state)]['time_diff'].sum())                                        \n",
    "            row['CT_deployment'] = format_timedelta(_status_change[_status_change['to'].isin(deployment_state)]['time_diff'].sum())\n",
    "            row['CT_cancelled'] = format_timedelta(_status_change[_status_change['to'].isin(cancel_state )]['time_diff'].sum())   \n",
    "\n",
    "            # current_status['QA'] = format_timedelta(_status_change[_status_change['from'].isin( ['Testing']) ] [_status_change['to'].isin( ['Deploy', 'Active'])]['time_diff'].sum())                    \n",
    "            # display(_status_change)\n",
    "            # display(row[['key','CT_leadTime', 'CT_development','CT_discovery', 'CT_deployment'  ]])\n",
    "    l_Df= l_Df.drop(columns=['Changelog'])\n",
    "    return l_Df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Df = calculateCycleTime(g_Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Upload file to sharepoint...\")\n",
    "\n",
    "try:\n",
    "    g_Df[g_Df['issueType'] == 'Portfolio Initiative'].to_csv(os.path.join(filepath, 'Portfolio_Initiative.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Product Initiative'].to_csv(os.path.join(filepath, 'Product_Initiative.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Epic'].to_csv(os.path.join(filepath, 'Epic.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Story'].to_csv(os.path.join(filepath, 'Story.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Task'].to_csv(os.path.join(filepath, 'Task.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Sub-task'].to_csv(os.path.join(filepath, 'Subtasks.csv'), index=False)\n",
    "    g_Df[g_Df['issueType'] == 'Test'].to_csv(os.path.join(filepath, 'Test.csv'), index=False)\n",
    "    g_Df.loc[g_Df['issueType'].isin(['Bug','Epic','Incident','Production Defects','Defect','Issue'])].to_csv(os.path.join(filepath, 'Bug.csv'), index=False)\n",
    "except Exception as e:  \n",
    "    logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "logger.info(\"Files uploaded\")\n",
    "logger.info(\"Script ended accumulating data from JIRA\")\n",
    "logger.info(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15/* * * * *  /usr/bin/python3 /Users/u1002018/src/PAE/JIRA/py/sole_proj.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def getLinkedIssues(key):\n",
    "#     check= epic_Df[epic_Df['key'] == key].Issuelinks.astype(bool)\n",
    "#     if check.to_list()[0] == True:\n",
    "#         issueLink_str=epic_Df[epic_Df['key'] == key]['Issuelinks']\n",
    "#         issueLink_str=issueLink_str.to_list()[0][0]\n",
    "\n",
    "#         searchQry=f'?jql= issue in linkedIssues(\"{key}\")'\n",
    "#         searchQry_DS = getDataSet(searchQry)\n",
    "#         linkedIssue=pd.json_normalize(searchQry_DS)\n",
    "\n",
    "#         specficfields = linkedIssue[['key','fields.status.name','fields.priority.name','fields.summary']]\n",
    "#         specficfields=specficfields.rename(columns={'fields.status.name': 'status','fields.priority.name':'priority', 'fields.summary':'summary'})\n",
    "#         json_specficfields =specficfields.to_json(orient='records', lines=True)\n",
    "#         # print(json_specficfields)\n",
    "#         return json_specficfields\n",
    "#     else:\n",
    "#         return \"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
