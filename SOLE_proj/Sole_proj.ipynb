{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common.ipynb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_token=os.getenv('JIRA_TOKEN')\n",
    "\n",
    "if not api_token:\n",
    "    raise ValueError(\"No JIRA API token found. Check your .env file / environment variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run_proj = sys.argv[1]\n",
    "run_proj ='SOLE'\n",
    "\n",
    "if run_proj == 'SOLE':\n",
    "    print('SOLE')\n",
    "    partitionCnt=1\n",
    "    filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/SOLE/'\n",
    "else:\n",
    "    print('ALL')\n",
    "    partitionCnt=30\n",
    "    filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/AllProjects/'\n",
    "\n",
    "filepath='./output/'\n",
    "\n",
    "\n",
    "datestr = \"\"\n",
    "\n",
    "\n",
    "discovery_state = [\"created\",\n",
    "    \"New\",\n",
    "    \"Ready for Development\",\n",
    "    \"PO Validation\",\n",
    "    \"PO Grooming\",\n",
    "    \"Ready for Refinement\",\n",
    "    \"To Do\",\n",
    "    \"FA Grooming\",\n",
    "    \"Open\",\n",
    "    \"In Planning\",\n",
    "    \"Dev Grooming/Estimation\",\n",
    "    \"Technical Grooming\",\n",
    "    \"Deferred\",\n",
    "    \"Backlog\",\n",
    "    \"Scrum of Scrums Items\",\n",
    "    \"Refinement\",\n",
    "    \"Ready for Assignment\",\n",
    "    \"Selected for Development\",\n",
    "    \"M Editorial Request Drafts\",\n",
    "    \"Pending Evaluation\",\n",
    "    \"Add to Backlog\",\n",
    "    \"Prioritized\",\n",
    "    \"Intake\",\n",
    "    \"Request Acknowledged\",\n",
    "    \"Design\",\n",
    "    \"Intake Request\",\n",
    "    \"Initiated\",\n",
    "    \"Assigned\",\n",
    "    \"Triage\",\n",
    "    \"Functional Grooming\",\n",
    "    \"Style Review\",\n",
    "    \"Acceptance / Review\",\n",
    "    \"Hold for Dependencies\",\n",
    "    \"Awaiting Requirements\",\n",
    "    \"Action Needed\",\n",
    "    \"On-Hold\",\n",
    "    \"Ready For Deployment\",\n",
    "    \"Grooming\",\n",
    "    \"Analysis in Progress\",\n",
    "    \"Waiting for Information\",\n",
    "    \"M Editorial To Do\",\n",
    "    \"M Editorial In Progress\",\n",
    "    \"S&D Request Drafts\",\n",
    "    \"S&D To Do\",\n",
    "    \"SM Backlog\",\n",
    "    \"Discovery Gate Prep\",\n",
    "    \"CAR Submitted\",\n",
    "    \"Ready for Requirements\",\n",
    "    \"Req-Arch in Progress\",\n",
    "    \"Ready for Estimation\",\n",
    "    \"Ready for Planning\",\n",
    "    \"Ready for Review\",\n",
    "    \"Planning in Progress\",\n",
    "    \"Architect Review\",\n",
    "    \"New Project Requests\",\n",
    "    \"Approved Project Backlog\",\n",
    "    \"Discovery\",\n",
    "    \"Architect Review Dependencies\",\n",
    "    \"JDAP Ready\",\n",
    "    \"Cleaned Up (Temporary)\",\n",
    "    \"With Vendor\",\n",
    "    \"S&D Stakeholder Review\",\n",
    "    \"Ready for test\",\n",
    "    \"First Priority\",\n",
    "    \"Results Ready\",\n",
    "    \"Analysis\",\n",
    "    \"ToDo\",\n",
    "    \"Ready\",\n",
    "    \"Integration Test\",\n",
    "    \"Approved & Prioritized\",\n",
    "    \"Discussion/Review\",\n",
    "    \"Pending\",\n",
    "    \"Ready to Work\",\n",
    "    \"Functional\",\n",
    "    \"NEEDS MORE INFO\",\n",
    "    \"IN UX/UI\",\n",
    "    \"Needs Approval\",\n",
    "    \"Assessing\",\n",
    "    \"Submitted for Assessment\",\n",
    "    \"Pending Decision\",\n",
    "    \"Decided\",\n",
    "    \"Ready for Dev\",\n",
    "    \"Action Required\",\n",
    "    \"Submitted\",\n",
    "    \"Planning\",\n",
    "    \"Measurement Requirements\",\n",
    "    \"UX Design\",\n",
    "    \"Stakeholder Review\",\n",
    "    \"Stakeholder Kickoff\",\n",
    "    \"Research\",\n",
    "    \"Analyze Results\",\n",
    "    \"Reviewed\",\n",
    "    \"Requirements\",\n",
    "    \"On Hold - To Do\",\n",
    "    \"Ice Box\",\n",
    "    \"Ready for Deploy\",\n",
    "    \"TO-DO\",\n",
    "    \"On Hold/Ready\",\n",
    "    \"Reseach\",\n",
    "    \"Wireframes To Do\",\n",
    "    \"New Requests\",\n",
    "    \"Approved for Estimation\",\n",
    "    \"Estimated / Ready for Review\",\n",
    "    \"Budgeted\",\n",
    "    \"Tech Feasibility\",\n",
    "    \"Product Review\",\n",
    "    \"Ready For Approval\",\n",
    "    \"New Request\",\n",
    "    \"Defect Created\",\n",
    "    \"Documentation and Tie-off\",\n",
    "    \"Business Case Review\",\n",
    "    \"Proposed\",\n",
    "    \"Under Evaluation\",\n",
    "    \"Under Review\",\n",
    "    \"Request Drafts\",\n",
    "    \"Parked for dependency\",\n",
    "    \"Parked\",\n",
    "    \"On Hold by FL\",\n",
    "    \"Begin\",\n",
    "    \"Concept\",\n",
    "    \"Change in Requirements\",\n",
    "    \"Ready For SNUG Review\",\n",
    "    \"Waiting Approval\",\n",
    "    \"Pending Acceptance Criteria\",\n",
    "    \"Wishlist\",\n",
    "    \"Approved Projects\",\n",
    "    \"Epics\",\n",
    "    \"Delayed\",\n",
    "    \"Not Started\",\n",
    "    \"Drafting\",\n",
    "    \"Pending PO Review\",\n",
    "    \"PO Acceptance\",\n",
    "    \"Rework\",\n",
    "    \"Ready for Implementation\",\n",
    "    \"SE Backlog\",\n",
    "    \"In UX/UI Review\",\n",
    "    \"Planning / Requirements\",\n",
    "    \"Ready for Dev Grooming & Estimation\",\n",
    "    \"Scoped\",\n",
    "    \"Ready to Diagnose\",\n",
    "    \"Waiting Feedback\",\n",
    "    \"To be reviewed\",\n",
    "    \"Board Review\",\n",
    "    \"Business Review\",\n",
    "    \"Wireframe IP\",\n",
    "    \"Mockup To Do\",\n",
    "    \"Wire Frame Needs Revision\",\n",
    "    \"Estimation\",\n",
    "    \"In Progress - UI\",\n",
    "    \"Priority List\",\n",
    "    \"Priority List - Ready for UI\",\n",
    "    \"UX\",\n",
    "    \"UX - Prioritized\",\n",
    "    \"PO - Need More Information\",\n",
    "    \"UX - In Progress\",\n",
    "    \"UX - On Hold (Waiting for Peer Review)\",\n",
    "    \"UX - Progress\",\n",
    "    \"PO - On Hold\",\n",
    "    \"Copy - In Progress\",\n",
    "    \"UI - Prioritized\",\n",
    "    \"PO - Unprioritized\",\n",
    "    \"Copy - Prioritized\",\n",
    "    \"UI-In Progress\",\n",
    "    \"UX - On Hold (Waiting for Results)\",\n",
    "    \"UX - On Hold (Waiting for Deploy)\",\n",
    "    \"UI - Ready for Style Review\",\n",
    "    \"In Progress - UX\",\n",
    "    \"Needs Approval - PO\",\n",
    "    \"UI - Needs Approval\",\n",
    "    \"On Hold - PO\",\n",
    "    \"Handed Off - Awaiting Dev\",\n",
    "    \"Prioritized - UXD\",\n",
    "    \"Design Idea\",\n",
    "    \"Product Owner / Stakeholder Review\",\n",
    "    \"Research Idea\",\n",
    "    \"Verification\",\n",
    "    \"WorkflowState\",      \n",
    "            ]\n",
    "development_state = [\"In Development\",\n",
    "    \"Code Review\",\n",
    "    \"On Hold\",\n",
    "    \"In Progress\",\n",
    "    \"Dev Blocked\",\n",
    "    \"Deploy to Staging\",\n",
    "    \"Dev Review\",\n",
    "    \"Blocked\",\n",
    "    \"Dev - In Progress\",\n",
    "    \"Dev Complete\",\n",
    "    \"Ready for Stage\",\n",
    "    \"Reopened\",\n",
    "    \"Build\",\n",
    "    \"In Review\",\n",
    "    \"Testing\",\n",
    "    \"Review\",\n",
    "    \"Work in Progress\",\n",
    "    \"Dev In Progress\",\n",
    "    \"Dev Complete-Pull Request\",\n",
    "    \"Development\",\n",
    "    \"S&D In Progress\",\n",
    "    \"S&D Blocked\",\n",
    "    \"Solution Review\",\n",
    "    \"Peer Review\",\n",
    "    \"Workaround\",\n",
    "    \"Needs Review\",\n",
    "    \"UA Review\",\n",
    "    \"Merge Request\",\n",
    "    \"Ready for Code Review\",\n",
    "    \"Approval Needed\",\n",
    "    \"PO Approval\",\n",
    "    \"Reopen\",\n",
    "    \"Ready for PR\",\n",
    "    \"Sprint Test\",\n",
    "    \"Demo\",\n",
    "    \"Remedy In Progress\",\n",
    "    \"In Design\",\n",
    "    \"Staged\",\n",
    "    \"Development in Progress\",\n",
    "    \"Merged\",\n",
    "    \"Reivew\",\n",
    "    \"In Code Review\",\n",
    "    \"Needs Style Review\",\n",
    "    \"Develop\",\n",
    "    \"Working\",\n",
    "    \"Style/Code Review\",\n",
    "    \"Dev Done\",\n",
    "    \"Pull Request Review\",\n",
    "    \"Executing\",\n",
    "    \"Needs Revision\",\n",
    "    \"Active\",\n",
    "    \"Wireframe Review\",\n",
    "    \"Mockup IP\",\n",
    "    \"Mockup Review\",\n",
    "    \"PO - Needs Approval\"]\n",
    "\n",
    "deployment_state = [\"Monitoring\",\n",
    "    \"Production Review\",\n",
    "    \"Ready for Prod\",\n",
    "    \"Deployed\",\n",
    "    \"Deploy\",\n",
    "    \"Validate/Verify\",\n",
    "    \"S&D Complete\",\n",
    "    \"Deployed to Production\",\n",
    "    \"Deploying Increments\",\n",
    "    \"Approved\",\n",
    "    \"Validation\",\n",
    "    \"Ready to Deploy\",\n",
    "    \"Reporter Approval\",\n",
    "    \"Implemented\",\n",
    "    \"Production\",\n",
    "    \"CR Submitted\",\n",
    "    \"Deploy to Production\",\n",
    "    \"In Production\",\n",
    "    \"Post-Production\",\n",
    "    \"Validation Complete\",\n",
    "    \"Deployed to Prod\",\n",
    "    \"Staged for Deploy\",\n",
    "    \"Released Live\",\n",
    "    \"Not a Bug\",\n",
    "    \"Ready for Production\", \n",
    "    \"Deploy to Prod\"]\n",
    "\n",
    "cancel_state = [\"Closed - Not Doing\",\n",
    "    \"Duplicate\",\n",
    "    \"Closed - No Deploy\",\n",
    "    \"Denied\",\n",
    "    \"Canceled\",\n",
    "    \"Rejected\",\n",
    "    \"Cancel\",\n",
    "    \"Cancelled\",\n",
    "    \"Not Approved / Out of Scope\",\n",
    "    \"Invalid\",\n",
    "    \"Failed\",\n",
    "    \"Closed - Inactivity\",\n",
    "    \"Closed - Duplicate\",]\n",
    "\n",
    "qa_state =[\"QA Testing\",\n",
    "    \"Future Test Cases\",\n",
    "    \"Deploy to Test\",\n",
    "    \"Deploy to UAT\",\n",
    "    \"QA (UAT)\",\n",
    "    \"QA Blocked\",\n",
    "    \"QA (Test)\",\n",
    "    \"Dev Complete / Deploy to Test\",\n",
    "    \"QA (Staging)\",\n",
    "    \"QA (Prod)\",\n",
    "    \"UA (Test)\",\n",
    "    \"SIT\",\n",
    "    \"Ready for QA\",\n",
    "    \"QA In Progress\",\n",
    "    \"Ready for UAT\",\n",
    "    \"QA Staging\",\n",
    "    \"In QA\",\n",
    "    \"UAT - In Progress\",\n",
    "    \"In Testing\",\n",
    "    \"QA\",\n",
    "    \"UAT\",\n",
    "    \"Test\",\n",
    "    \"Testing / Integration (Test)\",\n",
    "    \"Deploy to QA\",\n",
    "    \"QA - In Progress\",\n",
    "    \"UAT tesing\",\n",
    "    \"test1\",\n",
    "    \"Testing in UAT\",\n",
    "    \"Ready for Testing\",\n",
    "    \"UAT Validation\",\n",
    "    \"UAT - Testing\",\n",
    "    \"Dev QA\",\n",
    "    \"QA Automation\",\n",
    "    \"QA Icebox\",\n",
    "    \"Quality Assurance\",\n",
    "    \"Deployed to UAT\",\n",
    "    \"User Acceptance Testing\",\n",
    "    \"UAT - In Review\",\n",
    "    \"To Be Tested\",\n",
    "    \"Sandbox\",\n",
    "    \"UAT - Complete\",\n",
    "    \"Stores UAT\",\n",
    "    \"QA / Testing\"]\n",
    "\n",
    "close_state = [\"Closed\",\n",
    "    \"Complete\",\n",
    "    \"Done\",\n",
    "    \"Resolved\",\n",
    "    \"Pass\",\n",
    "    \"Fixed\",\n",
    "    \"Closed - Pending Review\",\n",
    "    \"Archived\",\n",
    "    \"Archive\",\n",
    "    \"Completed\",\n",
    "    \"Completed - Change(s) Made\",\n",
    "    \"Closed - Archive\",]\n",
    "\n",
    "\n",
    "if run_proj == 'SOLE' :\n",
    "    issueLists = [\n",
    "              {'issueType':\"Portfolio Initiative\",'partitionCnt':1},\n",
    "              {'issueType':\"Product Initiative\", 'partitionCnt':1 },\n",
    "              {'issueType':\"Epic\",'partitionCnt':5},\n",
    "              {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Story\",'partitionCnt':partitionCnt}\n",
    "             ]\n",
    "else:\n",
    "    issueLists = [\n",
    "            #   {'issueType':\"Portfolio Initiative\",'partitionCnt':1},\n",
    "            #   {'issueType':\"Product Initiative\", 'partitionCnt':1 },\n",
    "            #   {'issueType':\"Epic\",'partitionCnt':5},\n",
    "            #   {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "              {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "            #   {'issueType':\"Story\",'partitionCnt':partitionCnt}\n",
    "             ]\n",
    "             \n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDataframe():\n",
    "    return pd.DataFrame (columns=['key','FixVersion', \n",
    "    'Priority',\n",
    "    'Acceptance Criteria',\n",
    "    'Severity Level',\n",
    "    'Labels',\n",
    "    'EpicLink',\n",
    "    # 'Issuelinks',\n",
    "    'Status',\n",
    "    'Components',\n",
    "    'IssueCreator',\n",
    "    'creator.emailAddress',\n",
    "    'creator.displayName',\n",
    "    #  'SubTasks',\n",
    "    'Reporter.name',\n",
    "    'reporter.emailAddress',\n",
    "    'reporter.displayName',\n",
    "    'issueType',\n",
    "    'projectKey',\n",
    "    'projectName',\n",
    "    'created_ts',\n",
    "    'updated_ts',\n",
    "    'ParentLink',\n",
    "    'summary',\n",
    "    'StoryPoint',\n",
    "    'Sprint', \n",
    "    'AssigneeName',\n",
    "    'AssigneeEmailAddress',\n",
    "    'AssigneeDisplayName',\n",
    "    'ResolutionDescription',\n",
    "    'ResolutionName',\n",
    "    'parent.Summary',\n",
    "    'parent.key',\n",
    "    'parent.status',\n",
    "    'parent.priority',\n",
    "    'Defect Type',\n",
    "    'Test Cycle',\n",
    "    'Bug Severity',\n",
    "    'Sub-track',\n",
    "    'RICE Object Type',\n",
    "    'Complexity',\n",
    "    'Scope',\n",
    "    'Baseline Scope',\n",
    "    'Sprint_state',\n",
    "    'Sprint_name',\n",
    "    'Sprint_startDate',\n",
    "    'Sprint_endDate',\n",
    "    'Sprint_completeDate',        \n",
    "    'Sprint_activatedDate',\n",
    "    'Sprint_sequence',\n",
    "    'Sprint_goal',\n",
    "    'Intial SOW',\n",
    "    'After Global Design',\n",
    "    'The Rudy Special',\n",
    "    'Changelog',\n",
    "    'CT_leadTime',\n",
    "    'CT_development',\n",
    "    'CT_discovery',\n",
    "    'CT_deployment',\n",
    "    'CT_cancelled',\n",
    "    'CT_qa',\n",
    "    'Changelog_Rec'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global logger\n",
    "logger = setup_logger()\n",
    "logger.info(\"=======================================\")\n",
    "logger.info(\"Script start accumulating data from JIRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(dataframe):\n",
    "    if dataframe is not None and not dataframe.empty:\n",
    "       dataframe= dataframe.filter(['key','fields.fixVersions',\n",
    "                    'fields.priority.name',\n",
    "                    'fields.customfield_31601',\n",
    "                    'fields.customfield_12402.value',\n",
    "                    'fields.labels',\n",
    "                    'fields.customfield_10006',\n",
    "                    # 'fields.issuelinks',\n",
    "                    'fields.status.name',\n",
    "                    'fields.components',\n",
    "                    'fields.creator.name',\n",
    "                    'fields.creator.emailAddress',\n",
    "                    'fields.creator.displayName',\n",
    "                    # 'fields.subtasks',\n",
    "                    'fields.reporter.name',\n",
    "                    'fields.reporter.emailAddress',\n",
    "                    'fields.reporter.displayName',\n",
    "                    'fields.issuetype.name',\n",
    "                    'fields.project.key',\n",
    "                    'fields.project.name',\n",
    "                    'fields.created',\n",
    "                    'fields.updated',\n",
    "                    'fields.customfield_12823',\n",
    "                    'fields.summary',\n",
    "                    'fields.customfield_10002',\n",
    "                    'fields.customfield_10004',\n",
    "                    'fields.assignee.name',\n",
    "                    'fields.assignee.emailAddress',\n",
    "                    'fields.assignee.displayName',\n",
    "                    'fields.resolution.description',\n",
    "                    'fields.resolution.name',\n",
    "                    'fields.parent.fields.summary',\n",
    "                    'fields.parent.key',\n",
    "                    'fields.parent.fields.status.name',\n",
    "                    'fields.parent.fields.priority.name',\n",
    "                    'fields.customfield_32101.value',\n",
    "                    'fields.customfield_32302.value',\n",
    "                    'fields.customfield_11605.value',\n",
    "                    'fields.customfield_32901',\n",
    "                    # 'fields.customfield_16902' ,\n",
    "                    'fields.customfield_32900.value' ,\n",
    "                    'fields.customfield_27503' ,\n",
    "                    'fields.customfield_16902.value',\n",
    "                    'fields.customfield_33000',\n",
    "                    'changelog.histories']).rename(columns={'fields.fixVersions': 'FixVersion', \n",
    "                                  'fields.priority.name':'Priority',\n",
    "                                  'fields.customfield_31601': 'Acceptance Criteria',\n",
    "                                  'fields.customfield_12402.value':'Severity Level',\n",
    "                                  'fields.labels':'Labels',\n",
    "                                  'fields.customfield_10006':'EpicLink',\n",
    "                                #   'fields.issuelinks': 'Issuelinks',\n",
    "                                  'fields.status.name': 'Status',\n",
    "                                  'fields.components':'Components',\n",
    "                                  'fields.creator.name':'IssueCreator',\n",
    "                                  'fields.creator.emailAddress':'creator.emailAddress',\n",
    "                                  'fields.creator.displayName': 'creator.displayName',\n",
    "                                  # 'fields.subtasks':'SubTasks',\n",
    "                                  'fields.reporter.name':'Reporter.name',\n",
    "                                  'fields.reporter.emailAddress':'reporter.emailAddress',\n",
    "                                  'fields.reporter.displayName': 'reporter.displayName',\n",
    "                                  'fields.issuetype.name':'issueType',\n",
    "                                  'fields.project.key':'projectKey',\n",
    "                                  'fields.project.name':'projectName',\n",
    "                                  'fields.created':'created_ts',\n",
    "                                  'fields.updated':'updated_ts',\n",
    "                                  'fields.customfield_12823': 'ParentLink',\n",
    "                                  'fields.summary':'summary',\n",
    "                                  'fields.customfield_10002':'StoryPoint',\n",
    "                                  'fields.customfield_10004':'Sprint',                                  \n",
    "                                  'fields.assignee.name':'AssigneeName',\n",
    "                                  'fields.assignee.emailAddress':'AssigneeEmailAddress',\n",
    "                                  'fields.assignee.displayName':'AssigneeDisplayName',\n",
    "                                  'fields.resolution.description':'ResolutionDescription',\n",
    "                                  'fields.resolution.name':'ResolutionName',\n",
    "                                  'fields.parent.fields.summary': 'parent.Summary',\n",
    "                                  'fields.parent.key': 'parent.key',\n",
    "                                  'fields.parent.fields.status.name': 'parent.status',\n",
    "                                  'fields.parent.fields.priority.name':'parent.priority',\n",
    "                                  'fields.customfield_32101.value':'Defect Type',\n",
    "                                  'fields.customfield_32302.value':'Test Cycle',\n",
    "                                  'fields.customfield_11605.value':'Bug Severity',\n",
    "                                  'fields.customfield_32901':'Sub-track',\n",
    "                                  'fields.customfield_32900.value' : 'RICE Object Type',\n",
    "                                  'fields.customfield_27503' : 'Complexity',\n",
    "                                  'fields.customfield_16902.value':'Scope',\n",
    "                                  'fields.customfield_33000':'Baseline Scope',\n",
    "                                  'changelog.histories':'Changelog'\n",
    "                                  })\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch( project, issueType):\n",
    "    global fetch_df \n",
    "    logger.info(f\"Fetch {issueType}...\")\n",
    "\n",
    "    if issueType == 'Story':\n",
    "        attribute =' &expand=projects.issuetypes.fields,changelog'\n",
    "    else:\n",
    "        attribute =' &expand=projects.issuetypes.fields'\n",
    "\n",
    "    searchQry=f'?jql= project in ({project}) and issueType=\"{issueType}\" and updated > startOfMonth(-11)'\n",
    "\n",
    "    # if cond_flg == True: \n",
    "    #     searchQry=f\"{searchQry} and status not in (Cancelled, Done)  {attribute}\"\n",
    "    # else:\n",
    "    searchQry=f\"{searchQry} {attribute}\"\n",
    "\n",
    "    fetch_df = getDataSet(searchQry,project, issueType, api_token )\n",
    "\n",
    "    return data_clean(fetch_df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSprintDetails(l_Df):\n",
    "    l_Df['Sprint'] = l_Df['Sprint'].apply(lambda x: convert_to_key_value(x))\n",
    "\n",
    "    l_Df['Sprint_state'] = l_Df['Sprint'].apply(lambda x: sprintvalue(x,'state') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_name']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'name') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_startDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'startDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_endDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'endDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_completeDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'completeDate') if x is not None else \"NA\" )\n",
    "    l_Df['Sprint_activatedDate']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'activatedDate') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_sequence']= epic_Df['Sprint'].apply(lambda x: sprintvalue(x,'sequence') if x is not None else \"NA\" )\n",
    "    # l_Df['Sprint_goal']= l_Df['Sprint'].apply(lambda x: sprintvalue(x,'goal') if x is not None else \"NA\" )\n",
    "    l_Df= l_Df.drop(columns=['Sprint'])\n",
    "\n",
    "    return l_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBaselineLst(element):\n",
    "   baselinelst =[]\n",
    "   if element != None:\n",
    "      if  element != 'NA':\n",
    "         for idx in element:\n",
    "            baselinelst.append(idx['value'])\n",
    "   return baselinelst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChangeLog(element):\n",
    "    changelog_Rec = [] \n",
    "    for history in element:\n",
    "        created_at = history.get('created',None)\n",
    "        for item in history['items']:\n",
    "            if item['field'] =='status' :\n",
    "                changelog_Rec.append({\n",
    "                    'created_at': created_at,\n",
    "                    'field': item['field'],\n",
    "                    'from': item['fromString'],\n",
    "                    'to': item['toString'],\n",
    "                })\n",
    "    return changelog_Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n):\n",
    "    \"\"\"Splits a list into n approximately equal parts.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    \n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timedelta(td):\n",
    "    \"\"\"Formats a timedelta object to 'day.hours:min:sec' format.\"\"\"\n",
    "\n",
    "    seconds = td.total_seconds()\n",
    "    days, seconds = divmod(seconds, 86400)\n",
    "    hours, seconds = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "\n",
    "    return f\"{int(days)}.{int(hours):02}:{int(minutes):02}:{int(seconds):02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_key_value(string):\n",
    "    result = {}\n",
    "   \n",
    "    if string :\n",
    "        pairs = string[0].split(',')  # Split string by comma\n",
    "        # logger.info(pairs)\n",
    "        idx =1\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')  # Split each pair by '='\n",
    "            result[key.strip()] = value.strip()  # Add to dictionary, stripping whitespace\n",
    "            idx = idx +1 \n",
    "            if idx > 9:\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCycleTime(l_Df):\n",
    "    cycleTime = {}\n",
    "    for index, row in l_Df.iterrows():\n",
    "        # display(row)\n",
    "        _status_change = pd.json_normalize(row['Changelog_Rec'])\n",
    "        if len(_status_change) > 0:\n",
    "            logger.debug(row['Status'])\n",
    "            if (row['Status'] not in close_state):\n",
    "                # Get the current time in UTC\n",
    "                now_utc = datetime.now(pytz.utc)\n",
    "                # Convert to Eastern Time\n",
    "                eastern = pytz.timezone('US/Eastern')\n",
    "                now_est = now_utc.astimezone(eastern)\n",
    "                new_node = {\n",
    "                            'created_at': now_est.strftime('%Y-%m-%dT%H:%M:%S.%f%z'),\n",
    "                             'field': 'status',\n",
    "                             'from': row['Status'],\n",
    "                             'to' : 'Active'\n",
    "                            #  'proj'=row['projectKey'],\n",
    "                            #  'key'=row['key']\n",
    "\n",
    "                            }\n",
    "                _status_change.loc[len(_status_change)] = new_node\n",
    "\n",
    "            newCreate_node = {\n",
    "                        # 'created_at': row['created_ts'].to_list()[0], \n",
    "                            'created_at': row['created_ts'],\n",
    "                            'field': 'status',\n",
    "                            'from': 'created',\n",
    "                            'to' : 'WorkflowState'\n",
    "                            # 'proj'=row['projectKey'],\n",
    "                            # 'key'=row['key']\n",
    "                        }\n",
    "\n",
    "            _status_change.loc[len(_status_change)] = newCreate_node   \n",
    "            _status_change.sort_values(by=['created_at'],inplace=True)\n",
    "            _status_change['created_at']= pd.to_datetime(_status_change['created_at'])\n",
    "            _status_change['time_diff'] = _status_change['created_at'].diff()\n",
    "\n",
    "\n",
    "\n",
    "            row['CT_leadTime'] = format_timedelta(_status_change['time_diff'].sum())\n",
    "\n",
    "            row['CT_discovery'] = format_timedelta(_status_change[_status_change['to'].isin( discovery_state )]['time_diff'].sum())\n",
    "            \n",
    "            row['CT_development'] = format_timedelta(_status_change[_status_change['to'].isin( development_state)]['time_diff'].sum())                                        \n",
    "            row['CT_deployment'] = format_timedelta(_status_change[_status_change['to'].isin(deployment_state)]['time_diff'].sum())\n",
    "            row['CT_cancelled'] = format_timedelta(_status_change[_status_change['to'].isin(cancel_state )]['time_diff'].sum())   \n",
    "            row['CT_qa'] = format_timedelta(_status_change[_status_change['to'].isin(qa_state)]['time_diff'].sum())\n",
    "\n",
    "            \n",
    "            _status_change['created_at'] = pd.to_datetime(_status_change['created_at'],utc=True)\n",
    "            _status_change['created_at'] = _status_change['created_at'].dt.date\n",
    "            _status_change['proj']=row['projectKey']\n",
    "            _status_change['key']=row['key']\n",
    "            \n",
    "            # display(_status_change)\n",
    "            output_path = os.path.join(filepath, 'transition_history_Story_working.csv')\n",
    "            _status_change.to_csv(output_path ,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "\n",
    "            \n",
    "    return l_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 09:14:18,952 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Portfolio Initiative\n",
      "2024-12-24 09:14:19,705 - root - WARNING - renaming Portfolio Initiative_working.csv TO Portfolio Initiative.csv....\n",
      "2024-12-24 09:14:19,706 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Product Initiative\n",
      "2024-12-24 09:14:20,544 - root - WARNING - renaming Product Initiative_working.csv TO Product Initiative.csv....\n",
      "2024-12-24 09:14:20,545 - root - WARNING - processing 0    SOLMerch\n",
      "dtype: object - Epic\n",
      "2024-12-24 09:14:29,375 - root - WARNING - processing 1    SOLEFIN\n",
      "dtype: object - Epic\n",
      "2024-12-24 09:14:38,642 - root - WARNING - renaming Epic_working.csv TO Epic.csv....\n",
      "2024-12-24 09:14:38,643 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Task\n",
      "2024-12-24 09:14:41,929 - root - WARNING - renaming Task_working.csv TO Task.csv....\n",
      "2024-12-24 09:14:41,930 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Sub-task\n",
      "2024-12-24 09:17:45,628 - root - WARNING - renaming Sub-task_working.csv TO Sub-task.csv....\n",
      "2024-12-24 09:17:45,629 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Bug\n",
      "2024-12-24 09:18:01,721 - root - WARNING - renaming Bug_working.csv TO Bug.csv....\n",
      "2024-12-24 09:18:01,723 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Incident\n",
      "2024-12-24 09:18:02,349 - root - ERROR - No records found (\"SOLMerch\", \"SOLEFIN\" - Incident)....\n",
      "2024-12-24 09:18:02,359 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Production Defects\n",
      "2024-12-24 09:18:02,770 - root - ERROR - No records found (\"SOLMerch\", \"SOLEFIN\" - Production Defects)....\n",
      "2024-12-24 09:18:02,771 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Defect\n",
      "2024-12-24 09:18:03,109 - root - ERROR - No records found (\"SOLMerch\", \"SOLEFIN\" - Defect)....\n",
      "2024-12-24 09:18:03,112 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Issue\n",
      "2024-12-24 09:18:03,733 - root - ERROR - No records found (\"SOLMerch\", \"SOLEFIN\" - Issue)....\n",
      "2024-12-24 09:18:03,735 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Test\n",
      "2024-12-24 09:18:43,908 - root - WARNING - renaming Test_working.csv TO Test.csv....\n",
      "2024-12-24 09:18:43,909 - root - WARNING - processing 0    SOLMerch\n",
      "1     SOLEFIN\n",
      "dtype: object - Story\n",
      "2024-12-24 09:23:05,765 - root - WARNING - renaming Story_working.csv TO Story.csv....\n",
      "2024-12-24 09:23:05,766 - root - WARNING - renaming transition_history_Story_working.csv TO transition_history_Story.csv....\n",
      "2024-12-24 09:23:05,766 - root - WARNING - Script ended accumulating data from JIRA \n",
      "2024-12-24 09:23:05,766 - root - WARNING - =======================================\n"
     ]
    }
   ],
   "source": [
    "# Get all .csv files in the 'data' directory\n",
    "csv_files = Path(filepath).glob(\"*_working.csv\")\n",
    "for file in csv_files:\n",
    "    filename=file.name\n",
    "    os.remove(os.path.join(filepath, filename))\n",
    "\n",
    "\n",
    "if run_proj == 'SOLE':\n",
    "   strings =pd.Series([\"SOLMerch\",\"SOLEFIN\"])\n",
    "else:\n",
    "   strings = projectList()\n",
    "\n",
    "for issueList in issueLists:\n",
    "    split_string_list = split_list(strings, issueList['partitionCnt'])\n",
    "    filtered_list = [series for series in split_string_list if not series.empty]\n",
    "    for partition in filtered_list:\n",
    "        logger.warning(f\"processing %s - %s\",partition,issueList['issueType'])\n",
    "        output_path = os.path.join(filepath, f\"{issueList['issueType']}_working.csv\")\n",
    "        df=fetch(', '.join(['\"{}\"'.format(value) for value in partition]),issueList['issueType'])\n",
    "        if df is not None and not df.empty:\n",
    "            g_Df = initDataframe()\n",
    "            g_Df = pd.concat([g_Df, df ], ignore_index=True)\n",
    "            g_Df = extractSprintDetails(g_Df)\n",
    "            g_Df=g_Df.fillna(\"NA\")\n",
    "\n",
    "            g_Df['FixVersion']= g_Df['FixVersion'].apply(lambda x: keyvalue(x,'name') if x is not None else \"\" )\n",
    "            if run_proj == 'SOLE':\n",
    "               g_Df['Baseline Scope']=g_Df['Baseline Scope'].apply(lambda x: getBaselineLst(x) )\n",
    "               g_Df['Intial SOW']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 1')] )\n",
    "               g_Df['After Global Design']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 2')] )\n",
    "               g_Df['The Rudy Special']= g_Df['Baseline Scope'].apply(lambda x: {True:'YES',False:'NO'}[x.__contains__('Baseline 3')] )\n",
    "            \n",
    "            # display(g_Df)\n",
    "            # g_Df['Issuelinks'] = g_Df['key'].apply(lambda x:getLinkedIssues(x))\n",
    "\n",
    "            logger.info(\"Files uploaded\")\n",
    "\n",
    "            logger.info(\"Working on Stories...\")\n",
    "            if issueList['issueType'] == 'Story':\n",
    "               g_Df['Changelog_Rec'] = g_Df['Changelog'].apply(lambda x: getChangeLog(x) )\n",
    "               g_Df = calculateCycleTime(g_Df)\n",
    "               if run_proj == 'SOLE':\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','Components'])\n",
    "               else:\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','summary','Components'])\n",
    "               try:\n",
    "                  g_Df[g_Df['issueType'] == 'Story'].to_csv(output_path,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "               except Exception as e:  \n",
    "                  logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "                  pass\n",
    "            else:\n",
    "               logger.info(\"Upload file to sharepoint...\")\n",
    "               if run_proj == 'SOLE':\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Changelog_Rec','Labels','Components'])\n",
    "               else:\n",
    "                  g_Df= g_Df.drop(columns=['Changelog','Labels','summary','Components'])\n",
    "               try:\n",
    "                  if issueList['issueType'] in ['Bug','Incident','Production Defects','Defect','Issue']:\n",
    "                     output_path = os.path.join(filepath, 'Bug_working.csv')\n",
    "                     g_Df.loc[g_Df['issueType'].isin(['Bug','Incident','Production Defects','Defect','Issue'])].to_csv(output_path, mode='a', header=not os.path.exists(output_path),index=False)\n",
    "                  else:                  \n",
    "                     g_Df[g_Df['issueType'] == issueList['issueType']].to_csv(output_path ,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "                  \n",
    "               except Exception as e:  \n",
    "                  logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "                  pass\n",
    "\n",
    "            logger.info(\"Move working files to current files\")\n",
    "            # Delete the old DataFrame \n",
    "            del(g_Df)\n",
    "\n",
    "            # Perform garbage collection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "            gc.collect()\n",
    "            # break\n",
    "    clean_folder(filepath,\"_working.csv\")\n",
    "logger.warning(f\"Script ended accumulating data from JIRA \")\n",
    "logger.warning(\"=======================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_Df[g_Df['Intial SOW']=='YES'][['Intial SOW','After Global Design','The Rudy Special','Baseline Scope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15/* * * * *  /usr/bin/python3 /Users/u1002018/src/PAE/JIRA/py/sole_proj.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fb6657aaf366e700473b4691a0218895a79fd82ea30347f6976080edd370586"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
