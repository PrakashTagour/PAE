{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common.ipynb\n",
    "import logging\n",
    "\n",
    "\n",
    "filepath='/Users/u1002018/Library/CloudStorage/OneDrive-SharedLibraries-FootLocker/Global Technology Services - DASH Doc Library/SOLE/'\n",
    "\n",
    "#filepath='./output/QA/'\n",
    "\n",
    "\n",
    "datestr = \"\"\n",
    "partitionCnt=1\n",
    "\n",
    "\n",
    "issueLists = [\n",
    "          {'issueType':\"Portfolio Initiative\",'partitionCnt':1},\n",
    "          {'issueType':\"Product Initiative\", 'partitionCnt':1 },\n",
    "          {'issueType':\"Epic\",'partitionCnt':5},\n",
    "          {'issueType':\"Task\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Sub-task\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Bug\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Incident\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Production Defects\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Defect\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Issue\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Test\",'partitionCnt':partitionCnt},\n",
    "          {'issueType':\"Story\",'partitionCnt':partitionCnt}\n",
    "            ]\n",
    "             \n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDataframe():\n",
    "    return pd.DataFrame (columns=['key','FixVersion', \n",
    "    'Priority',\n",
    "    'Acceptance Criteria',\n",
    "    'Severity Level',\n",
    "    'Labels',\n",
    "    'EpicLink',\n",
    "    'Issuelinks',\n",
    "    'Status',\n",
    "    'Components',\n",
    "    'issueType',\n",
    "    'projectKey',\n",
    "    'projectName',\n",
    "    'created_ts',\n",
    "    'updated_ts',\n",
    "    'ParentLink',\n",
    "    'summary'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global logger\n",
    "logger = setup_logger(logging.WARNING)\n",
    "logger.warning(\"=======================================\")\n",
    "logger.warning(\"Script start accumulating data from JIRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(dataframe):\n",
    "    if dataframe is not None and not dataframe.empty:\n",
    "       dataframe= dataframe.filter(['key','fields.fixVersions', \n",
    "                                  'fields.priority.name',\n",
    "                                  'fields.customfield_31601',\n",
    "                                  'fields.customfield_12402.value',\n",
    "                                  'fields.labels',\n",
    "                                  'fields.customfield_10006',\n",
    "                                  'fields.issuelinks',\n",
    "                                  'fields.status.name',\n",
    "                                  'fields.components',\n",
    "                                  'fields.issuetype.name',\n",
    "                                  'fields.project.key',\n",
    "                                  'fields.project.name',\n",
    "                                  'fields.created',\n",
    "                                  'fields.updated',\n",
    "                                  'fields.customfield_12823',\n",
    "                                  'fields.summary']).rename(columns={'fields.fixVersions': 'FixVersion', \n",
    "                                  'fields.priority.name':'Priority',\n",
    "                                  'fields.customfield_31601': 'Acceptance Criteria',\n",
    "                                  'fields.customfield_12402.value':'Severity Level',\n",
    "                                  'fields.labels':'Labels',\n",
    "                                  'fields.customfield_10006':'EpicLink',\n",
    "                                  'fields.issuelinks': 'Issuelinks',\n",
    "                                  'fields.status.name': 'Status',\n",
    "                                  'fields.components':'Components',\n",
    "                                  'fields.issuetype.name':'issueType',\n",
    "                                  'fields.project.key':'projectKey',\n",
    "                                  'fields.project.name':'projectName',\n",
    "                                  'fields.created':'created_ts',\n",
    "                                  'fields.updated':'updated_ts',\n",
    "                                  'fields.customfield_12823': 'ParentLink',\n",
    "                                  'fields.summary':'summary',\n",
    "                                                                    })\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch( project, issueType):\n",
    "    global fetch_df \n",
    "    logger.info(f\"Fetch {issueType}...\")\n",
    "\n",
    "\n",
    "    attribute =' &expand=projects.issuetypes.fields'\n",
    "\n",
    "    searchQry=f'?jql= project in ({project}) and issueType=\"{issueType}\" and updated > startOfMonth(-11)'\n",
    "\n",
    "    # if cond_flg == True: \n",
    "    #     searchQry=f\"{searchQry} and status not in (Cancelled, Done)  {attribute}\"\n",
    "    # else:\n",
    "    searchQry=f\"{searchQry} {attribute}\"\n",
    "\n",
    "    fetch_df = getDataSet(searchQry,project, issueType)\n",
    "\n",
    "    return data_clean(fetch_df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n):\n",
    "    \"\"\"Splits a list into n approximately equal parts.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    \n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_key_value(string):\n",
    "    result = {}\n",
    "   \n",
    "    if string :\n",
    "        pairs = string[0].split(',')  # Split string by comma\n",
    "        # logger.info(pairs)\n",
    "        idx =1\n",
    "        for pair in pairs:\n",
    "            key, value = pair.split('=')  # Split each pair by '='\n",
    "            result[key.strip()] = value.strip()  # Add to dictionary, stripping whitespace\n",
    "            idx = idx +1 \n",
    "            if idx > 9:\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIssueLinkNode(issue,df,type,direction):\n",
    "      dfi = pd.json_normalize(issue['fields'])\n",
    "      # dft = pd.json_normalize(type)\n",
    "      #   display(type)\n",
    "      node = {'key': df['key'],'IssueLinkKey': issue['key'], 'projectKey' : issue['key'].split('-')[0],\n",
    "            'status': dfi['status.name'][0], 'priority':dfi['priority.name'][0], \n",
    "            'issuetype':dfi['issuetype.name'][0], \n",
    "            'TypeName': type['name'], 'inward': type['inward'],'outward': type['outward'],'direction': direction}\n",
    "      # display(node)\n",
    "      return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIssueLink(df):\n",
    "    l_df = pd.DataFrame (columns=['key','IssueLinkKey', \n",
    "        'status',\n",
    "        'priority',\n",
    "        'issuetype',\n",
    "        'inward',\n",
    "        'outward',\n",
    "        'projectKey'\n",
    "        ])\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        lst = row['Issuelinks']\n",
    "        # itype = row['']\n",
    "\n",
    "        if len(lst) > 0:\n",
    "            idxlen = len(lst)\n",
    "            for rec in lst:\n",
    "                if rec.get(\"inwardIssue\"):\n",
    "                    inwardIssues = rec['inwardIssue']\n",
    "                    l_df = l_df.append(createIssueLinkNode(inwardIssues,row,rec['type'],'inward'), ignore_index=True)\n",
    "                if rec.get(\"outwardIssue\"):\n",
    "                    outwardIssues = rec['outwardIssue']\n",
    "                    l_df = l_df.append(createIssueLinkNode(outwardIssues,row,rec['type'],'outward'), ignore_index=True)\n",
    "    # display(l_df)\n",
    "    return l_df\n",
    "    # print (df['Issuelinks'].to_list()[0][idx]['inwardIssue']['fields'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all .csv files in the 'data' directory\n",
    "csv_files = Path(filepath).glob(\"*_working_QA.csv\")\n",
    "for file in csv_files:\n",
    "    filename=file.name\n",
    "    os.remove(os.path.join(filepath, filename))\n",
    "\n",
    "strings ='\"SOLMerch\"'\n",
    "\n",
    "# strings = projectList()\n",
    "\n",
    "for issueList in issueLists:\n",
    "    # split_string_list = split_list(strings, issueList['partitionCnt'])\n",
    "    # filtered_list = [series for series in split_string_list if not series.empty]\n",
    "    # for partition in filtered_list:\n",
    "        # logger.warning(f\"processing %s - %s\",partition,issueList['issueType'])\n",
    "\n",
    "    df=fetch('SOLMerch',issueList['issueType'])\n",
    "    if df is not None and not df.empty:\n",
    "        g_Df = initDataframe()\n",
    "        g_Df = pd.concat([g_Df, df ], ignore_index=True)\n",
    "        g_Df=g_Df.fillna(\"NA\")\n",
    "\n",
    "        g_Df['FixVersion']= g_Df['FixVersion'].apply(lambda x: keyvalue(x,'name') if x is not None else \"\" )\n",
    "\n",
    "        # display(g_Df)\n",
    "        # g_Df['Issuelinks']= g_Df.apply(lambda row: getIssueLink(row['Issuelinks'], row['key']) )\n",
    "\n",
    "\n",
    "        logger.info(\"Files uploaded\")\n",
    "        logger.info(\"Upload file to sharepoint...\")\n",
    "\n",
    "        try:\n",
    "            if issueList['issueType'] in ['Portfolio Initiative']:\n",
    "                output_path = os.path.join(filepath, 'Portfolio_Initiative_ParentEpic_QA_working.csv')\n",
    "            else:              \n",
    "                output_path = os.path.join(filepath, f\"IssueLink_QA_working.csv\")\n",
    "\n",
    "            i_df = getIssueLink(g_Df)\n",
    "            # display(i_df)\n",
    "            if (len(i_df.index) > 0):\n",
    "                i_df.to_csv(output_path ,mode='a', header=not os.path.exists(output_path), index=False)\n",
    "        except Exception as e:  \n",
    "            logger.error(f\"Failed to upload files to {filepath}: {e}\")\n",
    "            pass\n",
    "        \n",
    "        clean_folder(filepath,\"_QA_working.csv\")\n",
    "        # Delete the old DataFrame \n",
    "        del(g_Df)\n",
    "\n",
    "        # Perform garbage collection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        gc.collect()\n",
    "        \n",
    "logger.info(\"Story Files uploaded\")\n",
    "\n",
    "logger.info(\"Move working files to current files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(g_Df['Issuelinks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_Df[g_Df['Intial SOW']=='YES'][['Intial SOW','After Global Design','The Rudy Special','Baseline Scope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(g_Df[g_Df['key'] == 'SOLMERCH-3536']['Issuelinks'].to_list()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15/* * * * *  /usr/bin/python3 /Users/u1002018/src/PAE/JIRA/py/sole_proj.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
